# Unified project configuration (single source of truth)
data:
  raw_path: data/raw/scryfall.json
  processed_dir: data/processed
  language: "en"
  seq_len: 120 # 512 o 256
  layouts_allowed: ["normal"]
  exclude_complex_layouts: true
  special_tokens:
    - "<|startofcard|>"
    - "<|gen_card|>"
    - "<|endofcard|>"
  split:
    train_ratio: 0.8
    val_ratio: 0.1
    dedup_exact: true    # rimozione duplicati 1:1
    shuffle_before_split: true  # quasi sicuramente sì

constants:
  rarities: ["common", "uncommon", "rare", "mythic"]
  types: ["Creature", "Instant", "Sorcery", "Enchantment", "Artifact", "Planeswalker", "Land"]
  colors: ["W", "U", "B", "R", "G"]
  regex:
    mana_pattern: "^(\\{([WUBRG]|[0-9]+|X|[WUBRG]/[WUBRG])\\})+$"
    pt_pattern: "^[0-9X]+\\/[0-9X]+$"

mapping:
  enabled: true
  seed_path: "src/mapping/mapping_seed.csv"
  keywords_path: "src/mapping/keywords.json"
  default_theme: "Generic"
  default_character: "N/A"

validator:

  # Regola opzionale per il "hard" validator:
  # se true, controlla che i simboli di mana siano coerenti con il campo color
  color_mana_subset_rule: false
  
  # Soglia minima (in caratteri) per considerare il text "non vuoto" nel soft validator
  # (Es: se una carta ha meno di 20 caratteri di testo, viene segnata come invalida in so
  soft_min_text_chars: 20

  # Se true, sia hard sia soft richiedono la presenza del campo "character:"
  require_character: true
  
  # Modalità di validazione da usare in GENERAZIONE (a06):
  # - "soft": più permissiva, prova a riparare name/mana_cost/text/pt (default raccomandata)
  # - "hard": usa le stesse regole rigide del dataset anche sulle carte generate
  mode_generation: "soft"

training:
  subset_fraction: 1   # usa solo 10% del set di train e validation
  eval_every_n_steps: 500
  save_every_n_steps: 2000
  run_name_prefix: "test_kaggle" #prefisso del run
  gpt2:
    model_name: "gpt2"
    mode: "full"
    lr_full: 2e-5
    lr_lora: 1e-4
    lora_r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    num_epochs: 3
    warmup_ratio: 0.02
    batch_eff: 64
    gradient_checkpointing: false
    weight_decay: 0.01
    batch_size: 1
    gradient_accumulation_steps: 4
  mistral:
    # modello base Mistral v0.3
    model_name: "mistralai/Mistral-7B-v0.3"
    # in alternativa, un checkpoint già 4bit tipo unsloth:
    # model_name: "unsloth/mistral-7b-v0.3-bnb-4bit"  # opzionale :contentReference[oaicite:1]{index=1}

    batch_size: 2                 # per-device
    gradient_accumulation_steps: 16 # eff. batch ~64
    num_epochs: 1
    lr: 2e-4
    weight_decay: 0.0
    warmup_ratio: 0.02
    gradient_checkpointing: false
    lr_scheduler_type: "cosine"

    # ---- quantizzazione modulare ----
    quantization_bits: 4        # 4 = QLoRA classico, 8 = LLM.int8
    quant_type: "nf4"           # usato solo se quantization_bits == 4
    double_quant: true
    use_flash_attention_2: true
    llm_int8_threshold: 6.0     # usato solo se quantization_bits == 8
    llm_int8_has_fp16_weight: false

    # ---- LoRA ----
    lora_r: 32
    lora_alpha: 32
    lora_dropout: 0.05
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - gate_proj
      - up_proj
      - down_proj

    # optimizer consigliato per QLoRA
    optim: "paged_adamw_8bit"

ppl_eval:
  run_name: "test_1-gpt2"
  split: "test"
  subset_fraction: 0.02
  batch_size: 4

generation:
  model_type: "gpt2"        # "mistral" | "gpt2"
  base_model_name: "gpt2"   # o gpt2
  checkpoint_path: "outputs/test_full_data-gpt2"  # dir dell’adapter o FT
  device: "cuda"               # "cuda" | "cpu" | "auto"
  batch_size: 8
  samples_per_mapping: 100     # o calcolato da num_samples / #mapping rows
  num_samples: 1500
  temperature: 0.8
  top_p: 0.9
  repetition_penalty: 1.3
  max_new_tokens: 160
  output_dir: "outputs/generations"   # opzionale; default se la ometti outputs/generations
  eos_token: "<|endofcard|>"

curation:
  output_dir: "outputs/final_set"        # opzionale, default se manca
  final_set_size: 100                    # opzionale
  dedup_threshold: 0.2                   # opzionale, 0.1 più permissivo; soglia di “distanza normalizzata” che usiamo in curation per evitare quasi-duplicati tra le carte generate
  target_counts:
    rarity:
      common: 60
      uncommon: 30
      rare: 9
      mythic: 1
  balance_by: ["color", "type", "rarity"]

wandb:
  enabled: true            # true = attiva il logging; false = disattiva (il codice salta l'inizializzazione W&B)
  project: "magic_deck_generator"       # nome del progetto su W&B (usalo come “cartella” che raggruppa i run)
  run_name_prefix: "exp"   # prefisso per i nomi dei run; il codice può aggiungere timestamp/param per un nome unico
