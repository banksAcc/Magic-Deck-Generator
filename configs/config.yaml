# Unified project configuration (single source of truth)
data:
  raw_path: data/raw/scryfall.json
  processed_dir: data/processed
  language: "en"
  seq_len: 384 # 512 o 256
  layouts_allowed: ["normal"]
  exclude_complex_layouts: true
  special_tokens:
    - "<|startofcard|>"
    - "<|gen_card|>"
    - "<|endofcard|>"
  split:
    train_ratio: 0.8
    val_ratio: 0.1
    dedup_exact: true    # rimozione duplicati 1:1
    shuffle_before_split: true  # quasi sicuramente sì

constants:
  rarities: ["common", "uncommon", "rare", "mythic"]
  types: ["Creature", "Instant", "Sorcery", "Enchantment", "Artifact", "Planeswalker", "Land"]
  colors: ["W", "U", "B", "R", "G"]
  regex:
    mana_pattern: "^(\\{([WUBRG]|[0-9]+|X|[WUBRG]/[WUBRG])\\})+$"
    pt_pattern: "^[0-9X]+\\/[0-9X]+$"

mapping:
  enabled: true
  seed_path: "src/mapping/mapping_seed.csv"
  keywords_path: "src/mapping/keywords.json"
  default_theme: "Generic"
  default_character: "N/A"

validator:
  enforce_field_order: true
  require_eos: true
  forbid_pt_without_creature: true
  color_mana_subset_rule: true

training:
  subset_fraction: 0.1   # usa solo 10% del set di train e validation
  eval_every_n_steps: 500
  save_every_n_steps: 1000
  run_name_prefix: "test_1" #prefisso del run
  gpt2:
    model_name: "gpt2"
    mode: "full"
    lr_full: 2e-5
    lr_lora: 1e-4
    lora_r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    num_epochs: 3
    warmup_ratio: 0.02
    batch_eff: 64
    gradient_checkpointing: false
    weight_decay: 0.01
    batch_size: 1
    gradient_accumulation_steps: 4
  mistral:
    model_name: "mistralai/Mistral-7B-v0.1"
    quant: "nf4"
    lora_r: 32
    lora_alpha: 32
    lora_dropout: 0.05
    lr: 1e-4
    epochs: 2
    warmup_ratio: 0.02
    batch_eff: 64
    grad_checkpointing: true
    weight_decay: 0.01

ppl_eval:
  run_name: "gpt2-gpt2"
  split: "test"
  subset_fraction: 1.0
  batch_size: 4

generation:
  model_type: "gpt2"        # "mistral" | "gpt2"
  base_model_name: "mistralai/Mistral-7B-Instruct-v0.2"   # o gpt2-large
  checkpoint_path: "outputs/checkpoints/mistral7b_qLoRA"  # dir dell’adapter o FT
  device: "cuda"               # "cuda" | "cpu" | "auto"
  batch_size: 8
  samples_per_mapping: 100     # o calcolato da num_samples / #mapping rows
  num_samples: 1500
  temperature: 0.8
  top_p: 0.9
  repetition_penalty: 1.1
  max_new_tokens: 160
  eos_token: "<|endofcard|>"

curation:
  target_counts:
    rarity: {common: 60, uncommon: 30, rare: 9, mythic: 1}
  balance_by: ["color", "type", "rarity"]
  dedup_threshold: 0.2 # 0.1 più permissivo; soglia di “distanza normalizzata” che usiamo in curation per evitare quasi-duplicati tra le carte generate

wandb:
  enabled: false            # true = attiva il logging; false = disattiva (il codice salta l'inizializzazione W&B)
  project: "magic_deck_generator"       # nome del progetto su W&B (usalo come “cartella” che raggruppa i run)
  run_name_prefix: "exp"   # prefisso per i nomi dei run; il codice può aggiungere timestamp/param per un nome unico
